clear; clc;
load("agent.mat") 

% Simulation time option
Ts = 0.1;
Tf = 20;

% DDPG agent's obs, act option
obsInfo = agent.ObservationInfo;
actInfo = agent.ActionInfo;

% make env
mdl = "AIV_robot";
agentBlk = mdl + "/Robot Controller";
env = rlSimulinkEnv(mdl,agentBlk,obsInfo,actInfo);



% make actor, critic Networks
[critic,actor] = createNetworks(obsInfo,actInfo);

% actor, critic optimizer option settings
criticOptions = rlOptimizerOptions(LearnRate=1e-03,GradientThreshold=1);
actorOptions = rlOptimizerOptions(LearnRate=1e-03,GradientThreshold=1);

% agent optin settings
noiseOpts = rl.option.OrnsteinUhlenbeckActionNoise(...
    'InitialAction', [0 0.5]', ...
    'Mean', 0, ...
    'MeanAttractionConstant', 0.15, ...
    'StandardDeviation', 0.3, ...
    'StandardDeviationDecayRate', 0.1, ...
    'StandardDeviationMin', 0);
agentOpts = rlDDPGAgentOptions(...
    SampleTime = Ts, ...
    CriticOptimizerOptions = criticOptions, ...
    ActorOptimizerOptions = actorOptions, ...
    ExperienceBufferLength = 1e5, ...
    DiscountFactor = 0.99, ...
    MiniBatchSize = 128, ...
    NoiseOptions= noiseOpts);

% make agent 
agent = rlDDPGAgent(actor,critic,agentOpts);


% agent training optin setting
trainOpts = rlTrainingOptions(...
    'MaxEpisodes', 2000, ...
    'MaxStepsPerEpisode', Tf/Ts, ...
    'Verbose', true, ...
    'Plots','training-progress', ...
    'StopTrainingCriteria', 'EpisodeReward', ...
    'StopTrainingValue', 65, ...
    'SaveAgentCriteria', 'EpisodeReward', ...
    'SaveAgentValue', 500);

% training agent
trainingStats = train(agent, env, trainOpts);
